{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb87599c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from dataset_PA1.dataloader import Dataloader\n",
    "from dataset_PA1.dataloader import datasetIterator\n",
    "from CNN_classes import Conv2DLayer\n",
    "from CNN_classes import MaxPoolingLayer\n",
    "from CNN_classes import LinearLayer\n",
    "from CNN_classes import ReLULayer\n",
    "from CNN_classes import softmax\n",
    "from CNN_classes import cross_entropy_loss\n",
    "from CNN_classes import softmax_cross_entropy_backward\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7aa4e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeLayerCNN:\n",
    "    def __init__(self, batch_size, input_channels, H_in, W_in, conv1_F, conv2_F, kernel_size, pool_size, learning_rate):\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        input_shape = (batch_size, input_channels, H_in, W_in)\n",
    "        self.conv1 = Conv2DLayer(input_shape=input_shape, out_channels=conv1_F, kernel_size=kernel_size, stride=1, padding=1, learning_rate=learning_rate)\n",
    "        self.relu1 = ReLULayer()\n",
    "        self.pool1 = MaxPoolingLayer(pool_size=pool_size, stride=pool_size)\n",
    "        mid_shape = (batch_size, conv1_F, H_in // pool_size, W_in // pool_size)\n",
    "        self.conv2 = Conv2DLayer(input_shape=mid_shape, out_channels=conv2_F, kernel_size=kernel_size, stride=1, padding=1, learning_rate=learning_rate)\n",
    "        self.relu2 = ReLULayer()\n",
    "        self.pool2 = MaxPoolingLayer(pool_size=pool_size, stride=pool_size)\n",
    "        \n",
    "        final_H = H_in // (pool_size * pool_size)\n",
    "        final_W = W_in // (pool_size * pool_size)\n",
    "        linear_input_size = conv2_F * final_H * final_W\n",
    "        output_size = 10\n",
    "        self.linear3 = LinearLayer(batch_size=batch_size, input_size=linear_input_size, output_size=output_size, learning_rate=learning_rate)\n",
    "\n",
    "        self.layers = [self.conv1, self.conv2, self.linear3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        L1 = self.conv1.forward(x)\n",
    "        A1 = self.relu1.forward(L1)\n",
    "        P1 = self.pool1.forward(A1)\n",
    "\n",
    "        L2 = self.conv2.forward(P1)\n",
    "        A2 = self.relu2.forward(L2)\n",
    "        P2 = self.pool2.forward(A2)\n",
    "        self.pool2_output_shape = P2.shape\n",
    "        P2_flat = P2.reshape(P2.shape[0], -1)\n",
    "\n",
    "        L3 = self.linear3.forward(P2_flat)\n",
    "        pred = softmax(L3)\n",
    "        return pred\n",
    "    \n",
    "    def backward(self, pred, ans):\n",
    "        dL3 = softmax_cross_entropy_backward(pred, ans)\n",
    "        dP2_flat = self.linear3.backward(dL3)\n",
    "        dP2 = dP2_flat.reshape(self.pool2_output_shape)\n",
    "        dA2 = self.pool2.backward(dP2)\n",
    "        dL2 = self.relu2.backward(dA2)\n",
    "        dP1 = self.conv2.backward(dL2)\n",
    "        dA1 = self.pool1.backward(dP1)\n",
    "        dL1 = self.relu1.backward(dA1)\n",
    "        self.conv1.backward(dL1)\n",
    "    \n",
    "    def update_params(self):\n",
    "        for layer in self.layers:\n",
    "            layer.update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1456a905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for image, label in data_loader:\n",
    "        label = np.argmax(label, axis=1)\n",
    "        pred = model.forward(image)\n",
    "        loss = cross_entropy_loss(pred, label)\n",
    "        total_loss += loss * image.shape[0]\n",
    "\n",
    "        pred_labels = np.argmax(pred, axis=1)\n",
    "        total_correct += np.sum(pred_labels == label)\n",
    "        total_samples += image.shape[0]\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "def predict_all(model, data_loader):\n",
    "    all_pred = []\n",
    "    all_label = []\n",
    "    for image, label in data_loader:\n",
    "        pred = model.forward(image)\n",
    "\n",
    "        all_pred.append(pred)\n",
    "        all_label.append(label)\n",
    "\n",
    "    final_pred = np.concatenate(all_pred, axis=0)\n",
    "    final_label = np.concatenate(all_label, axis=0)\n",
    "    return final_pred, final_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdb69b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(epochs, learning_rate, batch_size, train_loader, test_loader):\n",
    "\n",
    "    input_channels = 1\n",
    "    H_in = 28\n",
    "    W_in = 28\n",
    "    conv1_F = 16\n",
    "    conv2_F = 32\n",
    "    kernel_size = 3\n",
    "    pool_size = 2\n",
    "    model = ThreeLayerCNN(batch_size=batch_size, input_channels=input_channels, H_in=H_in, W_in=W_in, conv1_F=conv1_F, conv2_F=conv2_F, kernel_size=kernel_size, pool_size=pool_size, learning_rate=learning_rate)\n",
    "\n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    test_losses = []\n",
    "\n",
    "    print(\"Training start\")\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        start_time = time.time()\n",
    "        epoch_loss = 0\n",
    "        total_batches = len(train_loader)\n",
    "\n",
    "        for _, (image, label) in enumerate(train_loader) :\n",
    "            image = image.reshape(image.shape[0], 1, 28, 28)\n",
    "\n",
    "            label = np.argmax(label, axis=1)\n",
    "            pred = model.forward(image)\n",
    "            loss = cross_entropy_loss(pred, label)\n",
    "            epoch_loss += loss\n",
    "\n",
    "            model.backward(pred, label)\n",
    "            model.update_params()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / total_batches\n",
    "        train_losses.append(avg_epoch_loss)\n",
    "\n",
    "        test_loss, test_acc = evaluate(model, test_loader)\n",
    "        test_accuracies.append(test_acc)\n",
    "        test_losses.append(test_loss)\n",
    "        print(f\"Epoch {epoch} completed in {time.time() - start_time:.2f}s - Train Loss: {avg_epoch_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")\n",
    "    \n",
    "    print(\"Training finish\")\n",
    "\n",
    "    return model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9431102",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, test_losses):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(test_losses, label='Test Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Test Loss over Epochs')\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "def confusion_matrix(pred, label, num_class=10):\n",
    "    pred_labels = np.argmax(pred, axis=1)\n",
    "    true_labels = np.argmax(label, axis=1)\n",
    "\n",
    "    conf_matrix = np.zeros((num_class, num_class), dtype=int)\n",
    "    num_samples = len(true_labels)\n",
    "    for i in range(num_samples):\n",
    "        conf_matrix[true_labels[i], pred_labels[i]] += 1\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    return conf_matrix\n",
    "\n",
    "def get_top3(pred, image, label, num_class=10):\n",
    "    fig, axes = plt.subplots(3, num_class, figsize=(20, 6))\n",
    "    plt.suptitle(\"Top 3 Predicted Images for Each Class (3-Layer NN)\")\n",
    "    \n",
    "    for i in range(num_class):\n",
    "        scores = pred[:, i]\n",
    "        top3_i = np.argsort(scores)[::-1][:3]\n",
    "        \n",
    "        \n",
    "        for j, img_idx in enumerate(top3_i):\n",
    "            \n",
    "            score = scores[img_idx]\n",
    "\n",
    "            pred_class = i\n",
    "            real_class = label[img_idx]\n",
    "            \n",
    "            ax = axes[j, i]\n",
    "            \n",
    "            image_reshaped = image[img_idx].reshape(28, 28)\n",
    "            ax.imshow(image_reshaped, cmap='gray')\n",
    "            ax.axis('off')\n",
    "\n",
    "            ax.set_title(f\"{score:.2f}\", fontsize=8) \n",
    "            \n",
    "            if j == 0:\n",
    "                ax.set_xlabel(f\"Pred {pred_class}\\nTrue {real_class}\", fontsize=9, color='blue' if pred_class == real_class else 'red')\n",
    "            else:\n",
    "                ax.set_xlabel(f\"True {real_class}\", fontsize=9, color='blue' if pred_class == real_class else 'red')\n",
    "\n",
    "            if i == 0:\n",
    "                 ax.set_ylabel(f\"Rank {j+1}\", rotation=0, labelpad=15, fontsize=10)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acb7152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m Dataloader(path\u001b[38;5;241m=\u001b[39mdata_path, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m      6\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m Dataloader(path\u001b[38;5;241m=\u001b[39mdata_path, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[1;32m----> 8\u001b[0m cnn_model, train_losses, test_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mtrain_cnn\u001b[1;34m(epochs, learning_rate, batch_size, train_loader, test_loader)\u001b[0m\n\u001b[0;32m     24\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mreshape(image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m     26\u001b[0m label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(label, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m cross_entropy_loss(pred, label)\n\u001b[0;32m     29\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m, in \u001b[0;36mThreeLayerCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m L1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1\u001b[38;5;241m.\u001b[39mforward(x)\n\u001b[0;32m     25\u001b[0m A1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1\u001b[38;5;241m.\u001b[39mforward(L1)\n\u001b[1;32m---> 26\u001b[0m P1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpool1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m L2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2\u001b[38;5;241m.\u001b[39mforward(P1)\n\u001b[0;32m     29\u001b[0m A2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2\u001b[38;5;241m.\u001b[39mforward(L2)\n",
      "File \u001b[1;32mc:\\Users\\jalna\\OneDrive\\바탕 화면\\DGIST\\딥개\\CNN_classes.py:137\u001b[0m, in \u001b[0;36mMaxPoolingLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    134\u001b[0m                 max_val \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(window)\n\u001b[0;32m    135\u001b[0m                 output[n, c, h, w] \u001b[38;5;241m=\u001b[39m max_val\n\u001b[1;32m--> 137\u001b[0m                 max_h, max_w \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munravel_index(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m, window\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    138\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask[n, c, h_start \u001b[38;5;241m+\u001b[39m max_h, w_start \u001b[38;5;241m+\u001b[39m max_w] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\jalna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jalna\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path = './dataset_PA1'\n",
    "learning_rate = 0.01\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "train_loader = Dataloader(path=data_path, is_train=True, batch_size=batch_size)\n",
    "test_loader = Dataloader(path=data_path, is_train=False, batch_size=batch_size)\n",
    "\n",
    "cnn_model, train_losses, test_losses = train_cnn(epochs, learning_rate, batch_size, train_loader=train_loader, test_loader=test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea77dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred, gt = predict_all(cnn_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e7f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(train_losses, test_losses)\n",
    "conf_matrix = confusion_matrix(pred, gt, num_class=10)\n",
    "\n",
    "# Load test data again to show predictions\n",
    "test_images = []\n",
    "test_labels = []\n",
    "for image, label in test_loader:\n",
    "    test_images.extend(image)\n",
    "    \n",
    "    label = np.argmax(label, axis=1)\n",
    "    test_labels.extend(label)\n",
    "    \n",
    "\n",
    "test_images = np.concatenate(test_images, axis=0)\n",
    "test_labels = np.array(test_labels)\n",
    "print(test_labels.shape)\n",
    "print(\"\\n--- 클래스별 상위 3개 예측 --- (모델이 각 숫자로 가장 확신하는 이미지들)\")\n",
    "get_top3(pred, test_images, test_labels, num_class=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
